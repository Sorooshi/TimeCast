# Time Series Forecasting Package

**ğŸŒ Languages:** [English](README.md) | [Ğ ÑƒÑÑĞºĞ¸Ğ¹](README_ru.md) | [ÙØ§Ø±Ø³ÛŒ](README_fa.md)

A comprehensive PyTorch-based package for time series forecasting that implements multiple state-of-the-art deep learning models with automated hyperparameter tuning, experiment management, and robust result tracking. **Mathematically validated** against formal LaTeX formulation with complete dimensional correspondence.

## ğŸš€ Key Features

- **Multiple State-of-the-Art Models**: LSTM, TCN, Transformer, HybridTCNLSTM, MLP
- **Automated Hyperparameter Tuning**: Using Optuna for optimal parameter search
- **Experiment Management**: Organized experiment tracking with custom descriptions
- **3 Training Modes**: Streamlined workflow for different use cases
- **Robust Data Processing**: Clean, efficient preprocessing without artificial time features
- **Merchant Data Preprocessing**: Complete pipeline for transaction-to-timeseries conversion
- **Mathematical Validation**: LaTeX formulation compatibility verified
- **Comprehensive Logging**: Detailed file logging for debugging and analysis
- **Cross-Platform Support**: Robust directory creation across different operating systems
- **Rich Visualization**: Training curves and evaluation plots
- **Modular Architecture**: Clean, maintainable code structure

## ğŸ“ Mathematical Foundation

This package implements the time series forecasting formulation described in our research paper:

### Problem Formulation
Given merchant-level transaction data, we forecast total consumption using historical sequences:

**LaTeX Notation â†’ Implementation Mapping:**
- Historical sequence: $\mathcal{H}_t \in \mathbb{R}^{(k+1) \times N}$ â†” `(sequence_length, n_features)`
- Merchant consumption: $X_t \in \mathbb{R}^N$ â†” `merchant_features[t]`
- Target prediction: $y_t = \sum_{m=1}^N x_{m,t}$ â†” `np.sum(data[t])`

**âœ… Dimensional Compatibility Verified:**
```
LaTeX: ğ’½_t âˆˆ â„^{(k+1)Ã—N}  â†”  Implementation: (batch_size, sequence_length, n_features)
```

## ğŸ“Š Models Implemented

| Model | Description | Use Case | Paper Reference |
|-------|-------------|----------|------------------|
| **LSTM** | Long Short-Term Memory network | Sequential pattern learning | Hochreiter & Schmidhuber (1997) |
| **TCN** | Temporal Convolutional Network | Hierarchical feature extraction | Bai et al. (2018) |
| **Transformer** | Self-attention based model | Complex temporal dependencies | Vaswani et al. (2017) |
| **HybridTCNLSTM** | Combined TCN + LSTM | Best of both architectures | Custom Implementation |
| **MLP** | Multi-Layer Perceptron | Baseline comparison | Zhang et al. (1998) |

## ğŸ› ï¸ Installation

1. **Clone the repository:**
```bash
git clone https://github.com/Sorooshi/Time_Series_Forecasting.git
cd Time_Series_Forecasting
```

2. **Create and activate a virtual environment:**
```bash
python -m venv venv
source venv/bin/activate  # On Windows: venv\Scripts\activate
```

3. **Install dependencies:**
```bash
pip install -r requirements.txt
```

## ğŸ“– Usage

### Quick Start with Merchant Data

For merchant transaction data preprocessing (recommended starting point):

```bash
# Step 1: Run the preprocessing example
python example.py

# Step 2: Train models on preprocessed data with all arguments
python main.py --model Transformer \
               --data_name merchant_processed \
               --data_path data/merchant_processed.csv \
               --mode train \
               --train_tuned false \
               --experiment_description "merchant_baseline" \
               --n_trials 100 \
               --epochs 100 \
               --patience 25 \
               --sequence_length 5
```

### Command Line Interface

The package provides a comprehensive CLI with 3 distinct modes:

```bash
python main.py --model <MODEL_NAME> \
               --data_name <DATASET_NAME> \
               --mode <MODE> \
               --experiment_description <DESCRIPTION> \
               [additional options]
```

### ğŸ¯ Training Modes

| Mode | Description | When to Use |
|------|-------------|-------------|
| `tune` | Hyperparameter optimization only | First time with new data/model |
| `train` | Training with tuned (`--train_tuned true`) or default (`--train_tuned false`) parameters | Main training mode |
| `predict` | Load trained model and make predictions (`--predict_tuned true/false`) | Making predictions |
| `report` | Display comprehensive experiment analysis | Analysis and comparison |

### ğŸ“Š Report Mode Features

The report mode provides comprehensive analysis of your experiments with multiple view options:

```bash
python main.py --model <MODEL> --data_name <DATA> --mode report --report_type <TYPE>
```

#### Report Types Available:

| Report Type | Description | Shows |
|-------------|-------------|--------|
| `all` | Complete comprehensive report | Everything combined |
| `models` | Available trained models | Model status, completeness |
| `performance` | Performance comparison tables | Best metrics, rankings |
| `best` | Best performing configurations | Top 5 configurations with hyperparameters |
| `timeline` | Experiment timeline | Chronological experiment history |
| `files` | File paths and storage info | Directory structure, file sizes |

#### ğŸ” Report Examples:

**Show all available models:**
```bash
python main.py --model LSTM --data_name test_data --mode report --report_type models
```

**Performance comparison:**
```bash
python main.py --model LSTM --data_name test_data --mode report --report_type performance
```

**Complete analysis:**
```bash
python main.py --model LSTM --data_name test_data --mode report --report_type all
```

#### ğŸ“ˆ What Each Report Shows:

**ğŸ¤– Models Report:**
- Available trained models with status (Complete/Partial/No Weights)
- Tuned vs default model availability
- Model completeness statistics
- Experiment organization overview

**ğŸ“Š Performance Report:**
- Best performance by model type
- Detailed performance rankings
- Test loss, RÂ², and MAPE comparisons
- Performance trends across experiments

**ğŸ† Best Configurations Report:**
- Top 5 best performing configurations
- Key hyperparameters for best models
- Performance metrics for each configuration
- Hyperparameter recommendations

**â° Timeline Report:**
- Chronological experiment history
- File modification timestamps
- Experiment frequency analysis
- Date range summaries

**ğŸ“ Files Report:**
- Complete directory structure
- Hyperparameter files with sizes and dates
- Weight files with storage information
- Results files organization
- Total storage usage statistics

### ğŸ“‹ Arguments

#### Required Arguments
- `--model`: Model name (LSTM, TCN, Transformer, HybridTCNLSTM, MLP)
- `--data_name`: Dataset name (without .csv extension)

#### Optional Arguments
- `--data_path`: Full path to data file (default: data/{data_name}.csv)
- `--mode`: Training mode (default: train)
- `--experiment_description`: Custom experiment description (default: seq_len_{sequence_length})
- `--train_tuned`: Whether to use tuned parameters for training (true/false, default: true)
- `--predict_tuned`: Whether to use tuned model for prediction (true/false, default: true)
- `--report_type`: Type of report to show (all/models/performance/best/timeline/files, default: all)
- `--n_trials`: Hyperparameter tuning trials (default: 100)
- `--epochs`: Training epochs (default: 100)
- `--patience`: Early stopping patience (default: 25)
- `--sequence_length`: Input sequence length (default: 10)
- `--k_folds`: Number of folds for K-fold cross validation (default: 5)

### ğŸ”§ Important: Data Path Usage

**Common Mistake:** Don't point `--data_path` to a directory!

```bash
# âŒ WRONG - This will fail
python main.py --model LSTM --data_name my_data --data_path data/

# âœ… CORRECT - Specify the complete file path
python main.py --model LSTM --data_name my_data --data_path data/my_data.csv

# âœ… RECOMMENDED - Let the system auto-construct the path
python main.py --model LSTM --data_name my_data
# This automatically uses: data/my_data.csv
```

**Key Points:**
- `--data_path` expects a **file path**, not a directory
- If omitted, the system constructs: `data/{data_name}.csv`
- Always include the `.csv` extension when specifying `--data_path`

## ğŸª Merchant Data Preprocessing

### Preprocessing Pipeline (`example.py`)

Complete pipeline for converting raw merchant transaction data to time series format:

```bash
python example.py
```

**Pipeline Steps:**
1. **Load Transaction Data**: Raw transaction-level data loading
2. **Merchant Aggregation**: Group by time periods and merchants
3. **Contextual Features**: Add time-based features (seasonality, holidays, etc.)
4. **LaTeX Compatibility**: Ensure dimensional correspondence
5. **Validation**: Test with TimeSeriesPreprocessor

**Input Format:**
```csv
timestamp,merchant_id,customer_id,amount,day_of_week,hour,is_weekend,is_holiday,transaction_speed,customer_loyalty_score
2023-01-01 03:41:00,1,23,16.02,6,3,True,False,8.87,79.8
2023-01-01 06:28:00,4,25,99.56,6,6,True,False,5.9,48.8
...
```

**Output Format:**
```csv
date,merchant_1,merchant_2,merchant_3,merchant_4,merchant_5,hour,day_of_week,is_weekend,month,day_of_month,sin_month,cos_month,sin_hour,cos_hour,is_holiday
2023-01-01,454.17,207.98,216.56,460.11,644.78,0,5,1.0,1,1,0.0,1.0,0.0,1.0,1.0
2023-01-02,423.89,189.45,234.12,501.23,678.91,0,0,0.0,1,2,0.0,1.0,0.0,1.0,0.0
...
```

### ğŸ’¡ Example Workflows

#### 1. Complete Merchant Data Workflow

```bash
# Step 1: Preprocess merchant data
python example.py

# Step 2: Hyperparameter tuning
python main.py --model Transformer \
               --data_name merchant_processed \
               --mode tune \
               --experiment_description "merchant_baseline" \
               --n_trials 50 \
               --epochs 100 \
               --sequence_length 5

# Step 3: Train with tuned parameters (K-fold CV)
python main.py --model Transformer \
               --data_name merchant_processed \
               --mode train \
               --train_tuned true \
               --experiment_description "merchant_tuned" \
               --epochs 100 \
               --sequence_length 5

# Step 4: Compare with default parameters
python main.py --model Transformer \
               --data_name merchant_processed \
               --mode train \
               --train_tuned false \
               --experiment_description "merchant_default" \
               --epochs 100 \
               --sequence_length 5

# Step 5: Make predictions with tuned model
python main.py --model Transformer \
               --data_name merchant_processed \
               --mode predict \
               --predict_tuned true \
               --experiment_description "merchant_tuned" \
               --sequence_length 5

# Step 6: View all results
python main.py --model Transformer \
               --data_name merchant_processed \
               --mode report \
               --experiment_description "merchant_baseline"
```

#### 2. Quick Testing Workflow

```bash
# Quick test with default parameters
python main.py --model LSTM \
               --data_name my_data \
               --mode train \
               --train_tuned false \
               --experiment_description "quick_test" \
               --epochs 20 \
               --sequence_length 5
```

## ğŸ§ª Testing and Validation

### Mathematical Validation

Verify LaTeX formulation compatibility:

```bash
python test_preprocessing_validation.py
```

**Validates:**
- âœ… Dimensional correspondence: $(k+1) \times N$ â†” `(sequence_length, n_features)`
- âœ… Target calculation: $y_t = \sum_{m=1}^N x_{m,t}$ â†” `np.sum(...)`
- âœ… Preprocessing pipeline compatibility
- âœ… Integration with existing models

### Comprehensive Testing

Run full test suite:

```bash
# Move to test directory
cd Test

# Run comprehensive tests
python test_script.py

# Test feature dimensions  
python test_feature_dimensions.py

# Validate preprocessing
python test_preprocessing_validation.py
```

## ğŸ—‚ï¸ Project Structure

```
Time_Series_Forecasting/
â”œâ”€â”€ ğŸ“ data/                     # Data files
â”‚   â”œâ”€â”€ merchant_synthetic.csv  # Sample merchant data
â”‚   â”œâ”€â”€ merchant_processed.csv  # Preprocessed merchant data
â”‚   â””â”€â”€ your_data.csv
â”œâ”€â”€ ğŸ“ models/                   # Model implementations
â”‚   â”œâ”€â”€ __init__.py
â”‚   â”œâ”€â”€ base_model.py
â”‚   â”œâ”€â”€ lstm.py
â”‚   â”œâ”€â”€ tcn.py
â”‚   â”œâ”€â”€ transformer.py
â”‚   â”œâ”€â”€ hybrid_tcn_lstm.py
â”‚   â””â”€â”€ mlp.py
â”œâ”€â”€ ğŸ“ utils/                    # Utility modules
â”‚   â”œâ”€â”€ __init__.py
â”‚   â”œâ”€â”€ data_preprocessing.py    # Data loading and preprocessing
â”‚   â”œâ”€â”€ training.py             # Training and evaluation
â”‚   â”œâ”€â”€ file_utils.py           # File and directory management
â”‚   â”œâ”€â”€ visualization.py        # Plotting and visualization
â”‚   â”œâ”€â”€ config_manager.py       # Hyperparameter management
â”‚   â”œâ”€â”€ results_manager.py      # Results saving and loading
â”‚   â”œâ”€â”€ workflow_manager.py     # Training workflow orchestration
â”‚   â””â”€â”€ data_utils.py           # Data utilities
â”œâ”€â”€ ğŸ“ Test/                     # Testing and validation
â”‚   â”œâ”€â”€ test_script.py          # Comprehensive test suite
â”‚   â”œâ”€â”€ test_feature_dimensions.py  # Feature dimension testing
â”‚   â””â”€â”€ test_preprocessing_validation.py  # LaTeX compatibility validation
â”œâ”€â”€ ğŸ“ Results/                  # Training results and summaries
â”‚   â””â”€â”€ {model}/{mode}/{experiment}/
â”œâ”€â”€ ğŸ“ Hyperparameters/         # Tuned and saved parameters
â”‚   â””â”€â”€ {model}/{experiment}/
â”œâ”€â”€ ğŸ“ Predictions/             # Model predictions
â”‚   â””â”€â”€ {model}/{mode}/{experiment}/
â”œâ”€â”€ ğŸ“ Metrics/                 # Detailed evaluation metrics
â”‚   â””â”€â”€ {model}/{mode}/{experiment}/
â”œâ”€â”€ ğŸ“ History/                 # Training history (loss curves)
â”‚   â””â”€â”€ {model}/{mode}/{experiment}/
â”œâ”€â”€ ğŸ“ Plots/                   # Training visualizations
â”‚   â””â”€â”€ {model}/{mode}/{experiment}/
â”œâ”€â”€ ğŸ“ Logs/                    # Training logs and debugging info
â”‚   â””â”€â”€ {model}/
â”œâ”€â”€ example.py                  # Merchant data preprocessing pipeline
â”œâ”€â”€ main.py                     # Main entry point
â”œâ”€â”€ requirements.txt            # Python dependencies
â”œâ”€â”€ README.md                   # This file
â”œâ”€â”€ README_ru.md               # Russian version
â””â”€â”€ a01_TS_forecasting.tex     # LaTeX research paper
```

## ğŸ“Š Data Format

### Input Requirements
- **Format**: CSV file
- **Datetime**: Column named 'date' or 'timestamp' (optional)
- **Features**: Numerical columns representing your time series features
- **No Preprocessing Required**: The system handles normalization automatically

### Raw Transaction Data Structure (for `example.py`)
```csv
timestamp,merchant_id,customer_id,amount,day_of_week,hour,is_weekend,is_holiday,transaction_speed,customer_loyalty_score
2023-01-01 03:41:00,1,23,16.02,6,3,True,False,8.87,79.8
2023-01-01 06:28:00,4,25,99.56,6,6,True,False,5.9,48.8
...
```

### Processed Time Series Data Structure
```csv
date,merchant_1,merchant_2,merchant_3,merchant_4,merchant_5,hour,day_of_week,is_weekend,month,day_of_month,sin_month,cos_month,sin_hour,cos_hour,is_holiday
2023-01-01,454.17,207.98,216.56,460.11,644.78,0,5,1.0,1,1,0.0,1.0,0.0,1.0,1.0
2023-01-02,423.89,189.45,234.12,501.23,678.91,0,0,0.0,1,2,0.0,1.0,0.0,1.0,0.0
...
```

## ğŸ“ˆ Outputs and Results

### Organized Experiment Structure
Each experiment creates a complete directory structure:

```
Results/Transformer/tune/baseline_experiment/
â”œâ”€â”€ summary.json              # Complete experiment summary
â””â”€â”€ plots/
    â”œâ”€â”€ loss_plot.png         # Training/validation loss
    â”œâ”€â”€ r2_plot.png           # RÂ² score progression
    â””â”€â”€ mape_plot.png         # MAPE progression

History/Transformer/tune/baseline_experiment/
â””â”€â”€ training_history.csv     # Epoch-by-epoch training data

Predictions/Transformer/tune/baseline_experiment/
â”œâ”€â”€ val_predictions.csv      # Validation predictions vs targets
â””â”€â”€ test_predictions.csv     # Test predictions vs targets

Metrics/Transformer/tune/baseline_experiment/
â””â”€â”€ metrics.json            # Final evaluation metrics

Hyperparameters/Transformer/baseline_experiment/
â”œâ”€â”€ tune_parameters.json    # Parameters from tuning
â””â”€â”€ apply_parameters.json   # Parameters used in apply mode

Logs/Transformer/
â””â”€â”€ tuning_log_20250620_HHMMSS.txt  # Detailed training logs
```

### Key Metrics Tracked
- **Loss**: Mean Squared Error
- **RÂ² Score**: Coefficient of determination
- **MAPE**: Mean Absolute Percentage Error
- **Training History**: Complete epoch-by-epoch progression

## ğŸ”§ Advanced Features

### Mathematical Foundation
- **LaTeX Formulation**: Implements formal mathematical framework
- **Dimensional Validation**: Automatic dimensional correspondence checking
- **Target Consistency**: Validated target calculation: $y_t = \sum_{m=1}^N x_{m,t}$

### Experiment Management
- **Custom Descriptions**: Organize experiments with meaningful names
- **Automatic Fallback**: Uses sequence length if no description provided
- **Safe Naming**: Automatically handles special characters in experiment names

### Robust Architecture
- **Cross-Platform**: Works on Windows, macOS, and Linux
- **Error Handling**: Comprehensive error checking and reporting
- **Modular Design**: Easy to extend and modify
- **Clean Data Processing**: No artificial time features for better compatibility

### Logging and Debugging
- **File Logging**: Detailed logs saved for each tuning session
- **Trial Tracking**: Individual hyperparameter trial results
- **Progress Monitoring**: Real-time training progress
- **Error Tracking**: Comprehensive error logging

## ğŸš€ Performance Tips

1. **Start with example.py**: For merchant data, use the preprocessing pipeline
2. **Use tuning mode**: Use `--mode tune` for new datasets
3. **Use train with --train_tuned false**: For quick baselines and comparisons  
4. **Experiment descriptions**: Use meaningful names for organization
5. **Logging**: Check log files for detailed training information
6. **Cross-validation**: Results are automatically validated on separate test sets
7. **Mathematical validation**: Run `test_preprocessing_validation.py` to verify setup

## ğŸ¤ Contributing

1. Fork the repository
2. Create a feature branch (`git checkout -b feature/amazing-feature`)
3. Commit your changes (`git commit -m 'Add amazing feature'`)
4. Push to the branch (`git push origin feature/amazing-feature`)
5. Open a Pull Request

## ğŸ“ License

This project is licensed under the MIT License - see the [LICENSE](LICENSE) file for details.

## ğŸ“š Citation

If you use this package in your research, please cite:

```bibtex
@software{time_series_forecasting_2025,
  title = {Time Series Forecasting Package: A Comprehensive PyTorch Framework with LaTeX Formulation Validation},
  author = {Soroosh Shalileh},
  year = {2025},
  url = {https://github.com/Sorooshi/Time_Series_Forecasting},
  note = {Modular time series forecasting with automated hyperparameter tuning and mathematical validation}
}
```

## ğŸ“ Contact

**Author**: Soroosh Shalileh  
**Email**: sr.shalileh@gmail.com  
**GitHub**: [Sorooshi](https://github.com/Sorooshi)

---

*Built with â¤ï¸ for the time series forecasting community*
*Mathematically validated and research-ready âœ…*
